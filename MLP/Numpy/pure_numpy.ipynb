{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from __future__ import print_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,units=10):\n",
    "        super(Layer,self).__init__()\n",
    "        self.weights=np.random.randn(input.shape[1],units)\n",
    "        self.bias=np.zeros((units,))\n",
    "    def forward(self,inputs):\n",
    "        output=np.matmul(inputs,self.weights)+self.bias\n",
    "        return output\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self,input_units,output_units,learning_rate=0.1):\n",
    "        super(Dense,self).__init__()\n",
    "        self.learning_rate=learning_rate\n",
    "        self.weights=np.random.randn(input_units,output_units)\n",
    "        self.bias=np.zeros(output_units)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs  \n",
    "        return np.matmul(self.inputs,self.weights)+self.bias \n",
    "    def backward(self,grad_output):\n",
    "        \n",
    "        input_grad=np.dot(grad_output,np.transpose(self.weights))\n",
    "\n",
    "        grad_weights=np.transpose(np.dot(np.transpose(grad_output),self.inputs))      \n",
    "        grad_bias=np.sum(grad_output,axis=0)\n",
    "        self.weights=self.weights-self.learning_rate*grad_weights\n",
    "        self.bias=self.bias-self.learning*grad_bias\n",
    "        return input_grad \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu(x)=max(0,x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        return np.maximum(0,inputs)\n",
    "\n",
    "    def backward(self,out_grad):\n",
    "        relu_grad=self.inputs>0\n",
    "        return out_grad*relu_grad \n",
    "    def anotherback(self,out_grad):\n",
    "        return np.where((self.inputs>0,out_grad,0))    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Softmax_crossentropy:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "#     def __call__(self,logits,inputs):\n",
    "#         self.inputs=inputs\n",
    "#         self.logits=logits\n",
    "#         logits_for_ans=logits[np.arange(len(logits)),inputs]\n",
    "#         xentropy=-logits_for_ans+np.log(np.sum(np.exp(logits),axis=-1))\n",
    "#         return xentropy\n",
    "#     def backward(self):\n",
    "    \n",
    "#         ones_for_answers = np.zeros_like(self.logits)\n",
    "#         ones_for_answers[np.arange(len(self.logits)),self.inputs] = 1\n",
    "        \n",
    "#         softmax = np.exp(self.logits) / np.exp(self.logits).sum(axis=-1,keepdims=True)\n",
    "        \n",
    "#         return (- ones_for_answers + softmax) / self.logits.shape[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        self.sigmoid=np.exp(inputs)/(1+np.exp(inputs))\n",
    "        return self.sigmoid\n",
    "    def backward(self,grad):\n",
    "        return self.sigmoid*(1-self.sigmoid)*grad \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,x):\n",
    "        #np.expand_dims(np.exp(x).sum(axis=1),axis=1)\n",
    "        self.soft=np.exp(x)/np.exp(x).sum(axis=1)[:None]\n",
    "        return self.soft\n",
    "    def backward(self,grad):\n",
    "        return self.soft*(grad-(grad*self.soft).sum(axis=1)[:None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,x,y):\n",
    "        self.old_x=x.clip(min=1e-8,max=None)\n",
    "        self.old_y=y \n",
    "        return (np.where(y==1,-np.log(self.old_x),0)).sum(axis=1)\n",
    "    def backward(self):\n",
    "        return np.where(self.old_y==1,-1/self.old_x, 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,layers,cost):\n",
    "        self.layers=layers\n",
    "        self.cost=cost \n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x=layer.forward(x)\n",
    "        return x\n",
    "    def loss(self,x,y):\n",
    "        return self.cost(self.forward(x),y)\n",
    "    def backward(self):\n",
    "        grad=self.cost.backward()    \n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            grad = self.layers[i].backward(grad)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,lr,nb_epoch,data):\n",
    "    for epoch in range(nb_epoch):\n",
    "        running_loss = 0.\n",
    "        num_inputs = 0\n",
    "        for mini_batch in data:\n",
    "            inputs,targets = mini_batch\n",
    "            num_inputs += inputs.shape[0]\n",
    "            #Forward pass + compute loss\n",
    "            running_loss += model.loss(inputs,targets).sum()\n",
    "            #Back propagation\n",
    "            model.backward()\n",
    "            #Update of the parameters\n",
    "            for layer in model.layers:\n",
    "                if type(layer) == Linear:\n",
    "                    layer.weights -= lr * layer.grad_w\n",
    "                    layer.biases -= lr * layer.grad_b\n",
    "        print(f'Epoch {epoch+1}/{nb_epoch}: loss = {running_loss/num_inputs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_minibatches(batch_size=64):\n",
    "    tsfms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    trn_set = datasets.MNIST('.', train=True, download=True, transform=tsfms)\n",
    "    trn_loader = torch.utils.data.DataLoader(trn_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    data = []\n",
    "    for mb in trn_loader:\n",
    "        inputs_t,targets_t = mb\n",
    "        inputs = np.zeros((inputs_t.size(0),784))\n",
    "        targets = np.zeros((inputs_t.size(0),10))\n",
    "        for i in range(0,inputs_t.size(0)):\n",
    "            targets[i,targets_t[i]] = 1.\n",
    "            for j in range(0,28):\n",
    "                for k in range(0,28):\n",
    "                    inputs[i,j*28+k] = inputs_t[i,0,j,k]\n",
    "        data.append((inputs,targets))\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
