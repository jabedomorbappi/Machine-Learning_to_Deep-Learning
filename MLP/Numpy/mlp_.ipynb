{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 10:36:09.941949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-29 10:36:16.401023: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-29 10:36:16.401066: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-29 10:36:17.178023: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-29 10:36:25.205107: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-29 10:36:25.205426: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-29 10:36:25.205458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss=(y-y_head)**2\n",
    "\n",
    "dl_dy=2*(y-head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('IRIS.csv')\n",
    "species=dict(zip(list(data['species'].unique()),([0,1,2])))\n",
    "data['species'].replace(species,inplace=True)\n",
    "y=data.pop('species')\n",
    "y_dummy=pd.get_dummies(y)\n",
    "y_dummy\n",
    "X=data.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Softmax:\\n    def __call__(self,z):\\n        \"\"\"Computes softmax function.\\n        z: array of input values.\\n        Returns an array of outputs with the same shape as z.\"\"\"\\n        # For numerical stability: make the maximum of z\\'s to be 0.\\n        shiftz = z - np.max(z)\\n        exps = np.exp(shiftz)\\n        self.out =exps / np.sum(exps)\\n        return self.out\\n\\n\\n    def softmax_gradient(self,gradient):\\n        \"\"\"Computes the gradient of the softmax function.\\n        z: (T, 1) array of input values where the gradient is computed. T is the\\n        number of output classes.\\n        Returns D (T, T) the Jacobian matrix of softmax(z) at the given z. D[i, j]\\n        is DjSi - the partial derivative of Si w.r.t. input j.\\n        \"\"\"\\n        Sz = self.out\\n        # -SjSi can be computed using an outer product between Sz and itself. Then\\n        # we add back Si for the i=j cases by adding a diagonal matrix with the\\n        # values of Si on its diagonal.\\n        D = -np.outer(Sz, Sz) + np.diag(Sz.flatten())\\n        soft_grad=D*gradient\\n        return soft_grad'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class Softmax:\n",
    "    def __call__(self,z):\n",
    "        \"\"\"Computes softmax function.\n",
    "        z: array of input values.\n",
    "        Returns an array of outputs with the same shape as z.\"\"\"\n",
    "        # For numerical stability: make the maximum of z's to be 0.\n",
    "        shiftz = z - np.max(z)\n",
    "        exps = np.exp(shiftz)\n",
    "        self.out =exps / np.sum(exps)\n",
    "        return self.out\n",
    "\n",
    "\n",
    "    def softmax_gradient(self,gradient):\n",
    "        \"\"\"Computes the gradient of the softmax function.\n",
    "        z: (T, 1) array of input values where the gradient is computed. T is the\n",
    "        number of output classes.\n",
    "        Returns D (T, T) the Jacobian matrix of softmax(z) at the given z. D[i, j]\n",
    "        is DjSi - the partial derivative of Si w.r.t. input j.\n",
    "        \"\"\"\n",
    "        Sz = self.out\n",
    "        # -SjSi can be computed using an outer product between Sz and itself. Then\n",
    "        # we add back Si for the i=j cases by adding a diagonal matrix with the\n",
    "        # values of Si on its diagonal.\n",
    "        D = -np.outer(Sz, Sz) + np.diag(Sz.flatten())\n",
    "        soft_grad=D*gradient\n",
    "        return soft_grad'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    def __call__(self,z):\n",
    "        self.z=z\n",
    "        e = np.exp(z-np.max(z))\n",
    "        s = np.sum(e, axis=1, keepdims=True)\n",
    "        self.out=e/s\n",
    "        return self.out\n",
    "    def backward(self,dl):\n",
    "        self.Z=tf.Variable(self.z)\n",
    "        with tf.GradientTape() as tape:\n",
    "            s=tf.nn.softmax(self.Z)\n",
    "        soft_grad=tape.gradient(s,self.Z)\n",
    "        return soft_grad*dl    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class categorica_crosentropy:\n",
    "    def __call__(self,y_pred,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE:\n",
    "    def __call__(self,y_pred,y_true):\n",
    "        try:\n",
    "\n",
    "            self.y_pred=y_pred\n",
    "            self.y_true=y_true\n",
    "            m = y_true.shape[1]\n",
    "            y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        except:\n",
    "            self.y_pred=y_pred\n",
    "            self.y_true=y_true\n",
    "            m=1\n",
    "\n",
    "\n",
    "\n",
    "    # Calculating loss\n",
    "        loss = -1/m * (np.dot(y_true.T, np.log(y_pred)) + np.dot((1 - y_true).T, np.log(1 - y_pred)))\n",
    "\n",
    "        return loss\n",
    "        #y_true= n by 1\n",
    "        #y_pred=n by 1\n",
    "        #np.dot(y_true.T,np.log(y_pred)) =1 by n dot n by 1= 1 by 1\n",
    "    \n",
    "    def backprop(self):\n",
    "        return ((self.y_pred-self.y_true)/self.y_pred*(1-self.y_pred))\n",
    "\n",
    "  # outut= del L / del Z\n",
    "  #       \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def __call__(self,y_pred,y_true):\n",
    "        self.y_pred=y_pred\n",
    "        self.y_true=y_true \n",
    "        self.loss=np.mean(np.square(np.subtract(y_pred,y_true)))\n",
    "        return self.loss\n",
    "    def backprop(self):\n",
    "        n=self.y_pred.shape[0]\n",
    "        dl_dy=np.subtract(self.y_pred,self.y_true)/n \n",
    "        return dl_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "      \n",
    "    def __call__(self,Z):\n",
    "        \n",
    "        self.z=(1./(1+np.exp(-Z)))\n",
    "        return self.z\n",
    "    def sigmoid_backprop(self,output_gradient):\n",
    "\n",
    "        return (output_gradient*self.z*(1-self.z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __call__(self, input_):\n",
    "        self.input_ = input_\n",
    "        self.output = np.clip(self.input_, 0, None)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient):\n",
    "      # import pdb; pdb.set_trace()  # By the way, this is how you can debug\n",
    "      self.input_gradient = (self.input_ > 0) * output_gradient\n",
    "      return self.input_gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self,input_dim:int,units:int =1):\n",
    "        super(Linear,self).__init__()\n",
    "        \n",
    "        #self.weight=np.random.randn(input_dim,units)*np.sqrt(2./units)\n",
    "        #self.bias=np.zeros(units)\n",
    "        self.weight=np.random.randn(input_dim,units)\n",
    "        self.bias=np.zeros(units)\n",
    "        \n",
    "        ## here weight dimension is = dimension by 1  \n",
    "        # if dimension of feature vector is =nxd  and out vector is =1\n",
    "        # then weight vector shape is = dx1\n",
    "        # bias vector shape is =1\n",
    "        \n",
    "    def __call__(self,X):\n",
    "        # here our weight vector shape is = dx1 \n",
    "        # feature vector shape is         =nxd\n",
    "        # so output vector shape is       =nxd * dx1 =nx1 \n",
    "        # so multiplication must be feature vector * weight vector =nxd * dx1 =nx1\n",
    "        self.x=X\n",
    "       \n",
    "        output = X @ self.weight + self.bias\n",
    "        return output\n",
    "        \n",
    "        #return self.x@self.weight+self.bias  \n",
    "    \n",
    "    def backward(self,dl_da):\n",
    "        # dl/da comes from loss class \n",
    "        # shape of dl-da is= nx1 \n",
    "        # shape of x is =nxd\n",
    "        # output must be as weight shape =dx1\n",
    "        # X.T shape is dxn \n",
    "        # out shape is of dl-dw = dxn * nx1=dx1\n",
    "        # weights shape=dx1   dl_da shape is nx1\n",
    "        #dl_da -(nx1) * transpose of weight (1xd) =nxd    dl_dx for mlp \n",
    "        self.dl_dw=self.x.T@dl_da\n",
    "        self.dl_db=dl_da.sum(axis=0)\n",
    "        self.dx=dl_da@ self.weight.T # nxd\n",
    "        \n",
    " \n",
    "        return self.dx\n",
    "    def update(self,lr=0.01):\n",
    "        self.weight=self.weight-lr*self.dl_dw\n",
    "        self.bias=self.bias-lr*self.dl_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,input_dim,units=32,num_class=3):\n",
    "        super(Model,self)\n",
    "        self.linear1=Linear(input_dim,units)\n",
    "        self.linear2=Linear(units,3)\n",
    "        #self.linear3=Linear(64,64)\n",
    "        #self.linear4=Linear(64,num_class)\n",
    "\n",
    "        self.sigmoid=Sigmoid()\n",
    "        self.softmax=softmax()\n",
    "        self.relu=Relu()\n",
    "\n",
    "        self.loss=BCE()\n",
    "        \n",
    "        \n",
    "    def __call__(self,xb):\n",
    "        self.xb=xb\n",
    "        #self.activation=self.sigmoid(self.xb)\n",
    "        layer1=self.linear1(self.xb)\n",
    "        relu1=self.relu(layer1)\n",
    "        layer2=self.linear2(relu1)\n",
    "        #relu2=self.relu(layer2)\n",
    "        #layer3=self.linear3(relu2)\n",
    "        #relu3=self.relu(layer3)\n",
    "        #layer4=self.linear4(relu3)\n",
    "        out=self.softmax(layer2)\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def model_backward(self,out_gradient):\n",
    "        \n",
    "     \n",
    "        \n",
    "\n",
    "        soft_gradient=self.softmax.softmax_gradient(out_gradient)\n",
    "        #linear4_gradient=self.linear.backward(soft_gradient)\n",
    "        #relu3_gradient=self.relu.backward(linear4_gradient)\n",
    "        #linear3_gradient=self.linear.backward(relu3_gradient)\n",
    "        #relu2_gradient=self.relu.backward(linear3_gradient)\n",
    "        linear2_gradient=self.linear2.backward(soft_gradient)\n",
    "        relu1_gradient=self.relu.backward(linear2_gradient)\n",
    "        linear1_gradient=self.linear1.backward(relu1_gradient)\n",
    "    \n",
    "\n",
    "\n",
    "        return linear1_gradient\n",
    "    def accuracy(self,y_true,pred,threshold=0.5):\n",
    "        pred[pred > threshold] = 1\n",
    "        pred[pred <= threshold] = 0  \n",
    "        accuracy=accuracy_score(y_true,pred) \n",
    "        return accuracy \n",
    "        \n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.linear2.update(lr)\n",
    "        self.linear1.update(lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=Model(X.shape[1],units=32,num_class=3)\n",
    "y_pred=model(X)\n",
    "loss=MSE()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss=MSE()\n",
    "loss_value=loss(np.argmax(y_pred,axis=1),y)\n",
    "\n",
    "out_gradient=loss.backprop()\n",
    "out_gradient.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1=Linear(4,3)\n",
    "Z=linear1(X)\n",
    "soft=softmax()\n",
    "y_=soft(Z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.96"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss=MSE()\n",
    "loss_value=loss(np.argmax(y_,axis=1),y)\n",
    "loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c30d50df12e2ccfc06faccafb0acdb1a2da5363e814ef21e6c6a68595b7243a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
